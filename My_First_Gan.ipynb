{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "My_First_Gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ioannis-Karyotis/Barbershop-booking/blob/master/My_First_Gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOSnwFIM8MtM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "74830d8a-e8ee-40e1-bc12-4b298b2aea66"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pylab as plt\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, ReLU, LeakyReLU, Dense\n",
        "from keras.layers.core import Activation, Reshape\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.core import Flatten\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.datasets import cifar10\n",
        "from keras import initializers\n",
        "\n",
        "\n",
        "def build_cifar10_discriminator(ndf=64, image_shape=(32, 32, 3)):\n",
        "    \"\"\" Builds CIFAR10 DCGAN Discriminator Model\n",
        "    PARAMS\n",
        "    ------\n",
        "    ndf: number of discriminator filters\n",
        "    image_shape: 32x32x3\n",
        "\n",
        "    RETURN\n",
        "    ------\n",
        "    D: keras sequential\n",
        "    \"\"\"\n",
        "    init = initializers.RandomNormal(stddev=0.02)\n",
        "\n",
        "    D = Sequential()\n",
        "\n",
        "    # Conv 1: 16x16x64\n",
        "    D.add(Conv2D(ndf, kernel_size=5, strides=2, padding='same',\n",
        "                 use_bias=True, kernel_initializer=init,\n",
        "                 input_shape=image_shape))\n",
        "    D.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 2: 8x8x128\n",
        "    D.add(Conv2D(ndf*2, kernel_size=5, strides=2, padding='same',\n",
        "          use_bias=True, kernel_initializer=init))\n",
        "    D.add(BatchNormalization())\n",
        "    D.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 3: 4x4x256\n",
        "    D.add(Conv2D(ndf*4, kernel_size=5, strides=2, padding='same',\n",
        "                 use_bias=True, kernel_initializer=init))\n",
        "    D.add(BatchNormalization())\n",
        "    D.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 4:  2x2x512\n",
        "    D.add(Conv2D(ndf*8, kernel_size=5, strides=2, padding='same',\n",
        "                 use_bias=True, kernel_initializer=init))\n",
        "    D.add(BatchNormalization())\n",
        "    D.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Flatten: 2x2x512 -> (2048)\n",
        "    D.add(Flatten())\n",
        "\n",
        "    # Dense Layer\n",
        "    D.add(Dense(1, kernel_initializer=init))\n",
        "    D.add(Activation('sigmoid'))\n",
        "\n",
        "    print(\"\\nDiscriminator\")\n",
        "    D.summary()\n",
        "\n",
        "    return D\n",
        "\n",
        "\n",
        "def build_cifar10_generator(ngf=64, z_dim=128):\n",
        "    \"\"\" Builds CIFAR10 DCGAN Generator Model\n",
        "    PARAMS\n",
        "    ------\n",
        "    ngf: number of generator filters\n",
        "    z_dim: number of dimensions in latent vector\n",
        "\n",
        "    RETURN\n",
        "    ------\n",
        "    G: keras sequential\n",
        "    \"\"\"\n",
        "    init = initializers.RandomNormal(stddev=0.02)\n",
        "\n",
        "    G = Sequential()\n",
        "\n",
        "    # Dense 1: 2x2x512\n",
        "    G.add(Dense(2*2*ngf*8, input_shape=(z_dim, ),\n",
        "        use_bias=True, kernel_initializer=init))\n",
        "    G.add(Reshape((2, 2, ngf*8)))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 1: 4x4x256\n",
        "    G.add(Conv2DTranspose(ngf*4, kernel_size=5, strides=2, padding='same',\n",
        "          use_bias=True, kernel_initializer=init))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 2: 8x8x128\n",
        "    G.add(Conv2DTranspose(ngf*2, kernel_size=5, strides=2, padding='same',\n",
        "          use_bias=True, kernel_initializer=init))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 3: 16x16x64\n",
        "    G.add(Conv2DTranspose(ngf, kernel_size=5, strides=2, padding='same',\n",
        "          use_bias=True, kernel_initializer=init))\n",
        "    G.add(BatchNormalization())\n",
        "    G.add(LeakyReLU(0.2))\n",
        "\n",
        "    # Conv 4: 32x32x3\n",
        "    G.add(Conv2DTranspose(3, kernel_size=5, strides=2, padding='same',\n",
        "          use_bias=True, kernel_initializer=init))\n",
        "    G.add(Activation('tanh'))\n",
        "\n",
        "    print(\"\\nGenerator\")\n",
        "    G.summary()\n",
        "\n",
        "    return G\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    # load cifar10 data\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    # convert train and test data to float32\n",
        "    X_train = X_train.astype(np.float32)\n",
        "    X_test = X_test.astype(np.float32)\n",
        "\n",
        "    # scale train and test data to [-1, 1]\n",
        "    X_train = (X_train / 255) * 2 - 1\n",
        "    X_test = (X_train / 255) * 2 - 1\n",
        "    print(\"lala\")\n",
        "    return X_train, X_test\n",
        "\n",
        "\n",
        "def plot_images(images, filename):\n",
        "    h, w, c = images.shape[1:]\n",
        "    grid_size = ceil(np.sqrt(images.shape[0]))\n",
        "    images = abs((images.reshape(grid_size, grid_size, h, w, c)\n",
        "              .transpose(0, 2, 1, 3, 4)\n",
        "              .reshape(grid_size*h, grid_size*w, c)))\n",
        "    print(\"lala2\")\n",
        "    plt.figure(figsize=(16, 16))\n",
        "    plt.imsave(filename, images)\n",
        "    print(\"lala3\")\n",
        "\n",
        "\n",
        "def plot_losses(losses_d, losses_g, filename):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(8, 2))\n",
        "    axes[0].plot(losses_d)\n",
        "    axes[1].plot(losses_g)\n",
        "    axes[0].set_title(\"losses_d\")\n",
        "    axes[1].set_title(\"losses_g\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def train(ndf=64, ngf=64, z_dim=100, lr_d=2e-4, lr_g=2e-4, epochs=2,\n",
        "          batch_size=128, epoch_per_checkpoint=1, n_checkpoint_images=36):\n",
        "\n",
        "    X_train, _ = get_data()\n",
        "    image_shape = X_train[0].shape\n",
        "    print(\"image shape {}, min val {}, max val {}\".format(\n",
        "        image_shape, X_train[0].min(), X_train[0].max()))\n",
        "\n",
        "    # plot real images for reference\n",
        "    plot_images(X_train[:n_checkpoint_images], \"real_images.png\")\n",
        "\n",
        "    # build models\n",
        "    D = build_cifar10_discriminator(ndf, image_shape)\n",
        "    G = build_cifar10_generator(ngf, z_dim)\n",
        "\n",
        "    # define Discriminator's optimizer\n",
        "    D.compile(Adam(lr=lr_d, beta_1=0.5), loss='binary_crossentropy',\n",
        "              metrics=['binary_accuracy'])\n",
        "\n",
        "    # define D(G(z)) graph for training the Generator\n",
        "    D.trainable = False\n",
        "    z = Input(shape=(z_dim, ))\n",
        "    D_of_G = Model(inputs=z, outputs=D(G(z)))\n",
        "\n",
        "    # define Generator's Optimizer\n",
        "    D_of_G.compile(Adam(lr=lr_g, beta_1=0.5), loss='binary_crossentropy',\n",
        "                   metrics=['binary_accuracy'])\n",
        "\n",
        "    # get labels for computing the losses\n",
        "    labels_real = np.ones(shape=(batch_size, 1))\n",
        "    labels_fake = np.zeros(shape=(batch_size, 1))\n",
        "\n",
        "    losses_d, losses_g = [], []\n",
        "\n",
        "    # fix a z vector for training evaluation\n",
        "    z_fixed = np.random.uniform(-1, 1, size=(n_checkpoint_images, z_dim))\n",
        "\n",
        "    # training loop\n",
        "    for e in range(epochs):\n",
        "        print(\"Epoch {}\".format(e))\n",
        "        for i in range(len(X_train) // batch_size):\n",
        "\n",
        "            # update Discriminator weights\n",
        "            D.trainable = True\n",
        "\n",
        "            # Get real samples\n",
        "            real_images = X_train[i*batch_size:(i+1)*batch_size]\n",
        "            loss_d_real = D.train_on_batch(x=real_images, y=labels_real)[0]\n",
        "\n",
        "            # Fake Samples\n",
        "            z = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n",
        "            fake_images = G.predict_on_batch(z)\n",
        "            loss_d_fake = D.train_on_batch(x=fake_images, y=labels_fake)[0]\n",
        "\n",
        "            # Compute Discriminator's loss\n",
        "            loss_d = 0.5 * (loss_d_real + loss_d_fake)\n",
        "\n",
        "            # update Generator weights, do not update Discriminator weights\n",
        "            D.trainable = False\n",
        "            loss_g = D_of_G.train_on_batch(x=z, y=labels_real)[0]\n",
        "\n",
        "        losses_d.append(loss_d)\n",
        "        losses_g.append(loss_g)\n",
        "\n",
        "        if (e % epoch_per_checkpoint) == 0:\n",
        "            print(\"loss_d={:.5f}, loss_g={:.5f}\".format(loss_d, loss_g))\n",
        "            fake_images = G.predict(z_fixed)\n",
        "            print(\"\\tPlotting images and losses\")\n",
        "            plot_images(fake_images, \"fake_images_e{}.png\".format(e))\n",
        "            plot_losses(losses_d, losses_g, \"losses.png\")\n",
        "\n",
        "\n",
        "train()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "lala\n",
            "image shape (32, 32, 3), min val -1.0, max val 1.0\n",
            "lala2\n",
            "lala3\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "\n",
            "Discriminator\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 16, 16, 64)        4864      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 8, 8, 128)         204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 2, 2, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 2049      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 4,312,193\n",
            "Trainable params: 4,310,401\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "\n",
            "Generator\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 2048)              206848    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 4, 4, 256)         3277056   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 8, 8, 128)         819328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 16, 16, 64)        204864    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTr (None, 32, 32, 3)         4803      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 32, 3)         0         \n",
            "=================================================================\n",
            "Total params: 4,516,739\n",
            "Trainable params: 4,514,819\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "loss_d=0.19513, loss_g=3.13827\n",
            "\tPlotting images and losses\n",
            "lala2\n",
            "lala3\n",
            "Epoch 1\n",
            "loss_d=0.01926, loss_g=0.00295\n",
            "\tPlotting images and losses\n",
            "lala2\n",
            "lala3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}